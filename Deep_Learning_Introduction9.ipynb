{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ad5a0-3294-4d7c-a2e9-f1b3a41e3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3daa6e-7acf-4e54-b65b-461c81bb1a5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grundlegende Struktur von künstlichen neuronalen Netzwerken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfce236-4c28-40bb-8925-8e494700c9ea",
   "metadata": {},
   "source": [
    "**[Künstliche neuronale Netze](https://de.wikipedia.org/wiki/K%C3%BCnstliches_neuronales_Netz)** gehen in ihrer konzeptionellen Grundlagen auf die Arbeit von [Warren McCulloch](https://de.wikipedia.org/wiki/Warren_McCulloch) und [Walter Pitts](https://de.wikipedia.org/wiki/Walter_Pitts) zurück, die bereits $1943$ in Analogie zu Neuronen verknüpfte Netzwerke zur räumlichen Mustererkennung vorschlugen. $1958$ gelang [Frank Rosenblatt et al.](https://de.wikipedia.org/wiki/Frank_Rosenblatt) in Form des **[Perzeptrons](https://de.wikipedia.org/wiki/Perzeptron)** die erste praktische Umsetzung eines neuronalen Netzwerks. $1969$ führte die Kritik von [Marvin Minsky](https://de.wikipedia.org/wiki/Marvin_Minsky) an der Unfähigkeit, mit einfachen Perzeptrons nicht linear separierbare Probleme (wie zum Beispiel beim **[XOR-Problem](https://de.wikipedia.org/wiki/Exklusiv-Oder-Gatter)**)  zu lösen, zu einem zeitweisen Rückgang des Forschungsinteresses (dem sogenannten KI-Winter). Dies änderte sich in den $1980$er Jahren, als durch verschiedene Fortschritte in der KI-Forschung wie zum Beispiel der Methode der **[Backpropagation](https://de.wikipedia.org/wiki/Backpropagation)** gezeigt werden konnte, dass mehrschichtige Perzeptrons auch in der Lage sind, nicht linear separierbare Probleme zu bewältigen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516a956-32cc-4c53-b8be-cad17f5f0749",
   "metadata": {},
   "source": [
    "Wir haben an anderer Stelle im Einzelnen über die verschiedenen Arten des maschinellen Lernens - das **[unüberwachte Lernen](https://de.wikipedia.org/wiki/Un%C3%BCberwachtes_Lernen)**, **[überwachte Lernen](https://de.wikipedia.org/wiki/%C3%9Cberwachtes_Lernen)** und **[bestärkende Lernen](https://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen)** - gesprochen. Wir haben verschiedene Machine-Learning-Algorithmen vorgestellt, die bei den unterschiedlichen Lernarten zum Einsatz kommen. Neuronale Netze zeichnen sich insbesondere dadurch aus, dass sie bei entsprechender Vorbereitung in allen drei Arten des Lernens erfolgreich eingesetzt werden können. Diese universelle Verwendbarkeit erklärt auch den vermehrten Einsatz von künstlichen neuronalen Netzwerken in unterschiedlichsten Bereichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe70cda-9907-4257-9b93-4d6a67edcf53",
   "metadata": {},
   "source": [
    "In diesem Kapitel werden wir uns mit den Stärken und Schwächen von **neuronalen Netze** sowie deren Einsatzmöglichkeiten und der zugrunde liegenden mathematischen Formulierung beschäftigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b0457-da59-4a3d-852c-838d44b693a6",
   "metadata": {},
   "source": [
    "## Aufbau der Nervenzelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2672936-b73c-461d-b338-68898661e5ff",
   "metadata": {},
   "source": [
    "Sehen wir uns zuerst das Vorbild für neuronale Netze an: die Nervenzelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18318918-e301-4204-a27f-84035c2293e7",
   "metadata": {},
   "source": [
    "<img src=\"./images/realistische-neuronenanatomie_1284-68077.avif\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da45dd-2433-4d75-a238-6270e55cf966",
   "metadata": {},
   "source": [
    "Eine Nervenzelle besteht, wie in der oberen Abbildung dargestellt, aus **Dendriten**, **Soma** und **Axon**. Dabei nehmen die **Dendriten** Botenstoffe, die sogenannten Neurotransmitter auf wenn diese von angeregten benachbarten Nervenzellen ausgeschüttet werden. Die Verbindungen zwischen den Dendriten und dem Axon der vorhergehenden Zelle bezeichnet man als **Synapse**. Das Neuron besitzt ein Membranpotential das ein Auslösen der Weitergabe eines Nervenreizes zuerst einmal unterdrückt. Erst beim Überschreiten eines gewissen **Schwellenwerts der Anregung** wird ein **Aktionspotential** ausgelöst und ein Nervenreiz über das **Axon** an die nächste Zelle weitergegeben. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211343a-b2b6-43e5-ada4-60b665dab58f",
   "metadata": {},
   "source": [
    "Dabei gilt bei der Anregung des Neurons das sogenannte **[Alles-oder-nichts-Gesetz](https://de.wikipedia.org/wiki/Alles-oder-nichts-Gesetz)**, dass aussagt das entweder ein Reiz vollständig ausgelöst wird oder gar nicht.\n",
    "\n",
    "Dies lässt sich mathematisch mit der **Heaviside-Sprungfunktion** ausdrücken, die definiert ist als:\n",
    "\n",
    "$$H(x) \\begin{cases} x \\lt 0 \\cdots 0 \\\\ x \\ge 0 \\cdots 1 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e03873-1201-442f-b9c1-72617e313dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaviside-Sprungfunktion\n",
    "def heaviside(x):\n",
    "    return 0.5 * (np.sign(x) + 1)\n",
    "\n",
    "\n",
    "# Erzeuge Werte für x\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Berechne die Werte der Heaviside-Sprungfunktion\n",
    "y = heaviside(x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.plot(x, y, linewidth = 2.5, label=\"Heaviside-Sprungfunktion\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"H(x)\")\n",
    "plt.title(\"Heaviside-Sprungfunktion\")\n",
    "plt.ylim([-0.5,1.5])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99053508-0973-40df-85bf-d9a2699e09d6",
   "metadata": {},
   "source": [
    "Wir haben also biologisch gesehen einen **variablen Reiz**, einen **Schwellenwert** der überschritten werden muss und eine **Aktivierungsfunktion** zur Auslösung des Reizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66febc43-2f3b-472e-bc4e-58ff59a3ee9a",
   "metadata": {},
   "source": [
    "## Einfaches Perzeptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26054a57-4e6a-46bf-aa39-38ae1f93e2bf",
   "metadata": {},
   "source": [
    "Betrachten wir als nächstes den einfachsten Aufbau eines neuronalen Netzwerks, des ursprünglich von Rosenblatt vorgeschlagenen **Pezeptrons**. Dieses ist aus zwei Eingängen, dem Neuron selbst und einem Ausgang aufgebaut, wie dies in der folgenden Abbildung dargestellt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4262a93-a4d7-4785-9e3b-b52455aaa30b",
   "metadata": {},
   "source": [
    "<img src=\"./images/perceptron.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce1500-aad2-4949-883d-37e347ac2473",
   "metadata": {},
   "source": [
    "Die Struktur von **künstlichen neuronalen Netzwerken** folgt im Grundprinzip dem Aufbau eines **biologischen Nervensystems**. In neuronalen Netzen entsprechen Neuronen den **[künstlichen Neuronen](https://de.wikipedia.org/wiki/K%C3%BCnstliches_Neuron)** oder Knoten im Netzwerk. Diese Neuronen sind die grundlegenden Verarbeitungseinheiten. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc8ed7-7aeb-42d1-9917-30787ff08834",
   "metadata": {},
   "source": [
    "Künstliche neuronale Netzwerke bestehen aus einzelnen Neuronen die in sogenannten Schichten (*engl. Layers*) angeordnet sind. Dabei besteht die erste Schicht oder Eingabeschicht (*engl. Input Layer*) aus den Eingabewerten, gefolgt von weiteren Schichten von Neuronen, den sogenannten versteckten Schichten (*engl. Hidden Layers*) und schließlich einer Ausgabeschicht (*engl. Output Layer*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a793f5-c2fa-428b-bc74-b7dea7ec47cf",
   "metadata": {},
   "source": [
    "In Analogie zu biologischen Neuronen, die durch Anregung über einen gewissen Grenzwert aktiviert werden, Reize weiterzuleiten, wird bei neuronalen Netzwerken die gewichtete Summe der Inputs an verbundene Neuronen weitergegeben. Bezogen auf ein Neuron ergibt sich:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf590a3-8b00-43f6-a36c-c9a8f5c8a92e",
   "metadata": {},
   "source": [
    "$$ \\text{Input} = \\sum_{i=1}^N x_i w_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e262a-a316-4a4b-83c6-c47b0881314d",
   "metadata": {},
   "source": [
    "Dabei sind die $x_i$ die einzelnen Eingabewerte und die $w_i$ die jeweiligen Gewichtungen der $N$ Inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0d0a1-bb98-4d96-b5ba-39b75cba80f2",
   "metadata": {},
   "source": [
    "Im Äquivalent zu der biologischen Schwelle, ab der ein Neuron aktiviert wird, um ein Signal weiterzuleiten, können wir einen Schwellenwert $b$ hinzufügen, den sogenannten Bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c220b8-ccc6-4095-94d2-da7b98d0cde2",
   "metadata": {},
   "source": [
    "$$ \\text{Input} = \\sum_{i=1}^N x_i w_i + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431b997-1364-461a-a31d-befa75a04f80",
   "metadata": {},
   "source": [
    "Auf die auf diese Weise berechnete Ausgabe müssen wir noch normalerweise eine geeignete Aktivierungsfunktionen anwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19c84f-ba84-44d9-aea6-64650d2abc89",
   "metadata": {},
   "source": [
    "$$f_{\\text{Aktiv}}(\\text{Input}) = f_{\\text{Aktiv}}(\\sum_{i=1}^N x_i w_i + b) = \\text{Output}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1011ae-e633-491d-853e-2e82ac940e01",
   "metadata": {},
   "source": [
    "Die **Aktivierungsfunktion** bildet die gewichtete Summe der Eingabewerte auf einen bestimmten Wertebereich ab und dient dazu, die Ausgabe eines Neurons oder einer Schicht zu steuern. Sie entscheidet, ob und in welchem Maße ein Neuron aktiviert wird und welche Informationen an die nächsten Schichten weitergegeben werden. Im Weiterern ermöglichen geeignete Aktivierungsfunktionen, auch nicht lineare Zusammenhänge zu beschreiben. Auf die genaue Form von verschiedenen Aktivierungsfunktionen werden wir später zurückkommen und gehen für den Moment von linear weitergeleiteten Werten aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784f5aa-dc8b-45e9-9256-29b76b854dba",
   "metadata": {},
   "source": [
    "Also mathematisch betrachtet \n",
    "\n",
    "$$f_{\\text{Aktiv}}(\\text{Input}) = \\text{Input}$$ \n",
    "\n",
    "und damit gilt\n",
    "\n",
    "$$ \\text{Input} = \\sum_{i=1}^N x_i w_i + b = \\text{Output}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377df48-f918-4d63-a56a-05d90145182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abildung Gewichte und Bias\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "# Werte für Gewicht und Bias\n",
    "weights = [3, 2, 1]  # Gewichtswerte für den ersten Plot\n",
    "bias_values = [0]  # Bias für den ersten Plot (Bias = 0)\n",
    "weight_constant = 1  # Konstanter Wert für Gewicht im zweiten Plot\n",
    "bias_values_2 = [1, 0, -1]  # Verschiedene Bias-Werte für den zweiten Plot\n",
    "\n",
    "# Erstellen zweier Subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi = 600)\n",
    "\n",
    "# Plot für die Gewichtswerte mit Bias = 0\n",
    "for weight in weights:\n",
    "    ax1.plot([-2, -1, 0, 1, 2], [weight * x + 0 for x in [-2, -1, 0, 1, 2]], label=f\"Weight = {weight}\")\n",
    "ax1.set_title(\"Verschiedene Gewichte bei konstantem Bias\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y = weight * x + bias\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax1.set_xticks(np.arange(-2,3,1))\n",
    "ax1.set_yticks(np.arange(-6,7,1))\n",
    "ax1.axhline(0, 2, color='black',linewidth=1)\n",
    "ax1.axvline(0, 6, color='black',linewidth=1)\n",
    "ax1.annotate('', xy=(2, 0), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "ax1.annotate('', xy=(0, 6), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "\n",
    "\n",
    "# Plot für konstanten Weight = 1 und verschiedene Bias-Werte\n",
    "for bias in bias_values_2:\n",
    "    ax2.plot([-2, -1, 0, 1, 2], [weight_constant * x + bias for x in [-2, -1, 0, 1, 2]], label=f\"Bias = {bias}\")\n",
    "ax2.set_title(\"Verschiedene Bias bei konstantem Gewicht\")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y = weight * x + bias\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "ax2.set_xticks(np.arange(-2,3,1))\n",
    "ax2.set_yticks(np.arange(-4,7,1))\n",
    "ax2.axhline(0, 2, color='black',linewidth=1)\n",
    "ax2.axvline(0, 5, color='black',linewidth=1)\n",
    "ax2.annotate('', xy=(2, 0), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "ax2.annotate('', xy=(0, 5), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2d78c-3741-4f42-9734-3865b01fa79d",
   "metadata": {},
   "source": [
    "Sehen wir uns ein einfaches Beispiel in `Python` an, um einige der mathematischen Konzepte nachzuvollziehen, die bei neuronalen Netzwerken zum Einsatz kommen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16eba5-c326-44ea-836b-f124b40fe1d7",
   "metadata": {},
   "source": [
    "## Einfaches neuronales Netzwerk in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6deb2a5-eae4-437d-a0de-34927f5a3a4e",
   "metadata": {},
   "source": [
    "Wir versuchen zuerst die Eingabeschicht eines Netzwerks bestehend aus einem Neuron mit zwei Eingabewerten $x_1, x_2$, zwei Gewichten $w_1, w_2$ und einem Bias $b$ in `Python` zu modellieren. Dabei werden beim ersten Durchlauf des Netzwerks zuerst die Eingabewerte in die Eingabeschicht weitergegeben und die Gewichte und der Bias zufällig initialisiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb5641-429e-4d34-bb44-80958e2066a7",
   "metadata": {},
   "source": [
    "### Vorwärtsdurchlauf des Netzwerks in `Python` (engl. Forwardpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1babe-d72e-4b4c-8f19-804071167d85",
   "metadata": {},
   "source": [
    "Wir beginnen mit dem Erstellen der Eingabeschicht und modellieren die Berechnung des Outputs des ersten Neurons. Dies entspricht dem ersten Schritt der Forwardpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbd1b4-1056-4837-8de5-90145113ee15",
   "metadata": {},
   "source": [
    "$$\\text{Input} = \\sum_{i=1}^2 x_i w_i + b = x_1 w_1 + x_2 w_2 + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cca98f-6078-4459-b7f5-76bb97529037",
   "metadata": {},
   "source": [
    "### Feedforward Propagation - Eingabeschicht mit einem Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c264ee-7164-424e-9b2d-16e2dc24bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabewerte\n",
    "inputs = [5, 7]\n",
    "\n",
    "# Zufällig initialisierte Gewichte\n",
    "W1     = [6, 4]\n",
    "\n",
    "# Zufällig initialisierter Bias\n",
    "b1     = 3\n",
    "\n",
    "# Aktivierung\n",
    "A1 = inputs[0] * W1[0] + inputs[1] * W1[1] + b1\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c71a88-6cb1-4f76-83fe-89d0658a8cca",
   "metadata": {},
   "source": [
    "Verallgemeinern wir unser Modell auf zwei Neuronen in der Eingabeschicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa5040-b56d-4177-815c-3c873a5ae218",
   "metadata": {},
   "source": [
    "### Feedforward Propagation - Eingabeschicht mit zwei Neuronen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6526bc-4d55-4291-874a-3610974015d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abbildung - zwei Neuronen in der Eingabeschicht\n",
    "# Slides\n",
    "image_paths = [\"./images/w_b1.png\", \"./images/w_b2.png\", \"./images/w_b3.png\", \"./images/w_b4.png\", \"./images/w_b5.png\", \"./images/w_b6.png\", \"./images/w_b7.png\", \"./images/w_b8.png\"]\n",
    "\n",
    "# Auswahl der Bilder\n",
    "def show_image(index):\n",
    "    img = mpimg.imread(image_paths[index])\n",
    "    plt.figure(figsize=(2, 1), dpi = 600)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Slider-Widget erstellen\n",
    "slider = widgets.IntSlider(min=0, max=len(image_paths) - 1, step=1, description='Bild')\n",
    "widgets.interactive(show_image, index=slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc63dc1-98fd-4ddc-b142-04de5a71910d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eingabewerte\n",
    "inputs = [5, 7]\n",
    "\n",
    "# Zufällig initialisierte Gewichte für Inputs in Neuron 1\n",
    "W1     = [6, 4]\n",
    "\n",
    "# Zufällig initialisierter Bias für Inputs in Neuron 1\n",
    "b1     = 3\n",
    "\n",
    "# Zufällig initialisierte Gewichte für Inputs in Neuron 2\n",
    "W2     = [5, 3]\n",
    "\n",
    "# Zufällig initialisierter Bias für Inputs in Neuron 2\n",
    "b2     = -5\n",
    "\n",
    "A1 = inputs[0] * W1[0] + inputs[1] * W1[1] + b1\n",
    "     \n",
    "A2 = inputs[0] * W2[0] + inputs[1] * W2[1] + b2\n",
    "\n",
    "print('Ausgabe von Neuron 1:', A1)\n",
    "print('Ausgabe von Neuron 2:', A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f3949-f786-46a5-89a7-f18882b012ba",
   "metadata": {},
   "source": [
    "Dies läßt sich vereinfachen, indem wir alle Gewichte einer Schicht zu einer Matrix zusammenfassen und die Inputs und Biases als Spaltenvektoren angeben. Wir berechnen also folgendes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc73c1-e64f-40b1-9576-1072f91e7f45",
   "metadata": {},
   "source": [
    "$$\n",
    "A_1\n",
    "=\n",
    "W_1 \\cdot \\vec{X} + \\vec{b_1}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "6 & 4 \\\\\n",
    "5 & 3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "5  \\\\\n",
    "7 \n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "3  \\\\\n",
    "-5 \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "61 \\\\\n",
    "41\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be07c7-d57e-4361-aaac-6c6cbe5f5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabewerte\n",
    "X = np.array([[5],[7]])\n",
    "# Gewichte der Eingabeschicht als Matrix\n",
    "W1 = np.array([[6, 4],[5, 3]])\n",
    "# Biases der Eingabeschicht als Spaltenvektor\n",
    "b1 = np.array([[3],[-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c6d39-e61d-433d-ae17-6a9e7122b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe versteckte Schicht\n",
    "W1 @ X + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eae95a-9939-40b3-86bc-4be32d30731f",
   "metadata": {},
   "source": [
    "### Vektoren und Matrizen in `NumPy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f451ba1-5901-4c07-aded-567a17d022ae",
   "metadata": {},
   "source": [
    "In diesem Abschnitt beschäftigen wir uns mit dem Framework `NumPy`. `NumPy` ist auf  numerische Berechnungen in `Python` spezialisiert und deckt viele Bereiche der Mathematik ab. Sehen wir uns an wie wir `NumPy` verwenden können, um lineare Algebra betreiben. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4c493-5e12-413e-9990-022962a41169",
   "metadata": {},
   "source": [
    "### Vektoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fb826-6fed-4d8c-8cca-fd17aafb5ccd",
   "metadata": {},
   "source": [
    "Das **Skalarprodukt** zweier Vektoren $\\vec{a}=\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}$ und $\\vec{b}=\\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix}$ ist gegeben durch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8880f65-b260-4668-a857-3f1810f355e4",
   "metadata": {},
   "source": [
    "$$\\vec{a} \\cdot \\vec{b}=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix}\n",
    "=\n",
    "1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564a832-9ec6-4552-af67-9c80bd8e8a59",
   "metadata": {},
   "source": [
    "Wir können in `NumPy`-Arrays mit der Funktion `array()` erstellen, indem wir Zeilen und Spalten als Listen übergeben. Die beiden Vektoren $\\vec{a}$ und $\\vec{b}$ aus dem obigen Beispiel können wir folgendermaßen in `NumPy` anschreiben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d0951-308f-43ae-8b03-954e8c894925",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "\n",
    "b = np.array([4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72111827-7104-4dba-8736-d6c0e76a665d",
   "metadata": {},
   "source": [
    "In `NumPy` können wir das Skalarprodukt zweier Vektoren mit der Funktion `dot()` oder dem Symbol `@` berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2c41a-ad63-4a4c-9b06-ac5175124ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vektor- bzw. Matrizenmultiplikation\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64c530-503b-4034-bfff-da1ac33dae74",
   "metadata": {},
   "source": [
    "*Hinweis*: Ein $n$-dimensionales Array wird in `NumPy` ohne weitere Spezifizierung nicht im mathematischen Sinn in Zeilen- oder Spaltenvektor unterschieden sondern nach den sogenannten Broadcasting Regeln interpretiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5109cc4-653d-41a6-8d24-69205957b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756a836-a4e9-49e4-ad20-0750b21a3b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea087bc-b153-475a-8402-a463fb35d256",
   "metadata": {},
   "source": [
    "Um einen Vektor eindeutig in `NumPy` festzulegen können wir mit zusätzlichen eckigen Klammern die fehlende Dimension angeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf86a68-00a7-4e6b-be97-59847a74f1fa",
   "metadata": {},
   "source": [
    "#### Beispiel: Spaltenvektor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21356d-6c32-43b5-b385-9a8cbe38bfa5",
   "metadata": {},
   "source": [
    "$\n",
    "\\vec{a}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3fd2c8-2c3c-42b3-878b-25afb5a232e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_column = np.array([[1],[2],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c90ae5-f1e6-4d1f-8cf0-98796fb560dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_column.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e232db5-3230-4fd3-8e97-939bad6350a6",
   "metadata": {},
   "source": [
    "#### Beispiel: Zeilenvektor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692402f9-675a-4ebf-810d-703cd42c593f",
   "metadata": {},
   "source": [
    "$\n",
    "\\vec{b}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 4\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\vec{a}^T\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2180d-7498-4c6b-b175-dbe8b3df1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row = np.array([[1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972a062-288c-4fbe-88fe-d635e225f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639670d6-037e-4b9a-96c4-f9bc5b244f23",
   "metadata": {},
   "source": [
    "Wir können Vektoren in `NumPy` mit der Syntax `vector.T` transponieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2bf0b-0dcd-4c36-a807-8a56007d90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_column.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31e796-d65e-45fe-9e59-981beff6dcab",
   "metadata": {},
   "source": [
    "### Aufgabe: \n",
    "\n",
    "Berechnen Sie das Skalarprodukt $\\vec{a}^T \\cdot \\vec{b}$ für die Vektoren:\n",
    "\n",
    "$\n",
    "\\vec{a}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}$\n",
    ",\n",
    "$\n",
    "\\vec{b}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e1720-cd18-4d5f-ad69-a7e3d2f3ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "\n",
    "b = np.array([[4],[5],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19441c-70db-42fe-b0f6-cc7f4704ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0eea06-93e1-4bb4-b74e-66078f874ceb",
   "metadata": {},
   "source": [
    "### Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b61b92-9161-4e45-9062-5d35adbc9e2d",
   "metadata": {},
   "source": [
    "Eine Matrix setzt sich aus Spalten- beziehungsweise Zeilenvektoren zusammen, dabei besteht eine $(m \\times n)$-Matrix aus $m$ Zeilen und $n$ Spalten. Das untere Beispiel stellt also eine $(3 \\times 3)$ Matrix dar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f98aa5-0eac-47eb-97ce-4133219f3d45",
   "metadata": {},
   "source": [
    "$$A =\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb130f0-fe90-4609-b57d-ae56e374243d",
   "metadata": {},
   "source": [
    "Beachten Sie, dass Matrizen insofern eine Verallgemeinerung von Vektoren darstellen, da diese dem Spezialfall einer $(n \\times 1)$ Matrix entsprechen. Wir können die oben angegebene Matrix $A$ in `NumPy` anschreiben, indem wir Zeilen und Spalten als Liste von Listen übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabdc5a-10b0-45cc-b67f-c24166ede3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1],[2],[3]])\n",
    "\n",
    "A = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c4578-0e5e-4dc0-8adc-a5cd7aaa8568",
   "metadata": {},
   "source": [
    "### Multiplikation von Vektoren mit Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733707f4-63ef-44c6-ab51-aa2a25b021e3",
   "metadata": {},
   "source": [
    "$\\vec{a}=\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}$ , $A =\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234b435-5c0f-4688-82a9-9e05b225ec85",
   "metadata": {},
   "source": [
    "$A \\cdot \\vec{a} = \\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3 \\\\\n",
    "4 \\cdot 1 + 5 \\cdot 2 + 6 \\cdot 3 \\\\\n",
    "7 \\cdot 1 + 8 \\cdot 2 + 9 \\cdot 3\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "14 \\\\\n",
    "32 \\\\\n",
    "50\n",
    "\\end{pmatrix}\n",
    "$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e2e89-832a-434e-a813-b5eb60bb26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd73db-1e37-4308-9566-3573bd5cd103",
   "metadata": {},
   "source": [
    "### Dimension von Vektoren, Matrizen und Tensoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5f930-d15b-483c-9d03-412d4ca21777",
   "metadata": {},
   "source": [
    "### Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09813d-76cb-41d2-a4b1-8d449ca6f081",
   "metadata": {},
   "source": [
    "Matrizen besitzen Dimensionen entsprechend ihrer Anzahl an Zeilen $m$ und Spalten $n$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61819eff-41ef-42a1-8e59-5267218e17bd",
   "metadata": {},
   "source": [
    "#### Beispiel: $4 \\times 4$ Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1b629-523f-4a8d-a40f-ffed1f7d0869",
   "metadata": {},
   "source": [
    "$A =\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12 \\\\\n",
    "13 & 14 & 15 & 16\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8550eff-175f-4a66-a535-902534fd2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_matrix = np.array([[1,2,3,4],\n",
    "                    [5,6,7,8],\n",
    "                    [9,10,11,12],\n",
    "                    [13,14,15,16]])\n",
    "\n",
    "\n",
    "print('A_matrix:')\n",
    "print(A_matrix)\n",
    "print('')\n",
    "print('Dimensionen von A_matrix:',A_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f108e-adfc-4d8a-bc98-9be5e2f6552a",
   "metadata": {},
   "source": [
    "Ähnlich wie Vektoren können Matrizen auch durch das Vertauschen von Zeilen und Spalten transponiert werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d97c0a-bb45-4030-92ba-58151c9eca45",
   "metadata": {},
   "source": [
    "$A^T=\n",
    "\\begin{pmatrix}\n",
    "1 & 5 & 9 & 13 \\\\\n",
    "2 & 6 & 10 & 14 \\\\\n",
    "3 & 7 & 11 & 15 \\\\\n",
    "4 & 8 & 12 & 16\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e9e22-7e38-4b60-a0ca-07ee44229f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80951a-6e50-43bd-bd3f-7b541c273ad7",
   "metadata": {},
   "source": [
    "*Hinweis*: Damit Vektoren oder Matrizen miteinander multipliziert werden können muß der linke Vektor oder die linke Matrix gleich viele Spalten besitzen wie der rechte Vektor oder die rechte Matrix Zeilen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468f6da-4ef6-4a9f-903a-b55b551060ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplikation zwischen Vektor und Matrix ist nicht kommutativ\n",
    "print('Dimensionen von a:', a.shape)\n",
    "print('Dimensionen von A:', A.shape)\n",
    "#a @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57801ae9-029a-4258-89e7-6eceb0a71383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allgemein gilt für zwei Matrizen A * B mit Dimensionen A = (m, n), B = (o, p),\n",
    "# dass die Dimension der Spalten n von A gleich der Dimension der Zeilen o von B sein müssen\n",
    "print('Dimensionen von a.T:', a.T.shape)\n",
    "print('Dimensionen von A  :', A.shape)\n",
    "np.dot(a.T,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296befe1-9a14-41f5-b6b7-ccfcbf4845e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Aufgabe: \n",
    "\n",
    "Erstellen Sie die zwei Matrizen $A = \\begin{pmatrix}1 & 2 \\\\ 4 & 4 \\end{pmatrix}$ und $B=\\begin{pmatrix}5 & 6 \\\\ 7 & 8 \\end{pmatrix}$ in `NumPy` und berechnen Sie $A \\cdot B$ und $B \\cdot A$. Gilt $A \\cdot B = B \\cdot A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb7cc6-5099-46ed-abc5-6f1bbb61b3d5",
   "metadata": {},
   "source": [
    "check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3e562-e51c-46cb-a761-b403e6f47de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],[3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],[7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc369bf7-7318-4ed8-bfa8-4025e4fd0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5746154-3d6d-4e2d-887a-7b39bb9b523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d566c-4403-4ed5-8f76-29ec9c4627a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Einfaches neuronales Netzwerk mit Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6fa9a-56f4-4049-8a8d-ca6ddf2f497a",
   "metadata": {},
   "source": [
    "Wir versuchen aus den bisherigen theoretischen Überlegungen ein neuronales Netz mit drei Eingabewerten, drei Neuronen in der der ersten versteckten Schicht, vier Neuronen in der zweiten versteckten Schicht und zwei Neuronen in der Ausgabeschicht zu erstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303ef98-a6a5-4b1b-b9ae-4987d10be995",
   "metadata": {},
   "source": [
    "### Feedforward Propagation - Neuronales Netzwerk mit mehreren Schichten in Matrixdarstellung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78be025-1aec-4c27-b9a1-3d361b04caa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialisieren der Gewichte und Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5c8b3-66d4-4eb9-b912-2d207f9c9929",
   "metadata": {},
   "source": [
    "In der Praxis werden die Gewichte $W_i$ und Schwellenwerte $b_i$ eines neuronalen Netzes am Anfang der Trainingsphase mit Zufallswerten initialisiert. In `NumPy` ist es möglich mit der Funktion `random.rand(m, n)` ein Array der Dimension $(m \\times n)$ gefüllt mit standardnormalverteilten Zufallszahlen zu erstellen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966f5a8-7eda-40f2-9fe6-bf38975831d2",
   "metadata": {},
   "source": [
    "Wir erstellen also für dieses Netz einen Spaltenvektor mit den Eingabewerten $X$ der Dimension $(3 \\times 1)$, die Matrix $W_1$ der Gewichte der ersten versteckten Schicht der Dimension $(3 \\times 3$ und die zugehörigen Biases $b_1$ der Dimension $(3 \\times 1)$, die Matrix $W_2$ der Gewichte der zweiten versteckten Schicht der Dimension $(4 \\times 3$ und die zugehörigen Biases $b_2$ der Dimension $(4 \\times 1)$ und die Matrix der Gewichte der Ausgabeschicht $W_3$ der Dimension $(2 \\times 4)$ und die zugehörigen Biases $b_3$ der Dimension $(2 \\times 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3428c-1ccc-4ce9-b6af-5df9e0c86d18",
   "metadata": {},
   "source": [
    "<img src=\"./images/3_3_4_2_netz.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df72da9-a1f3-41fc-b673-1b06075a6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3  # Eingabeschicht\n",
    "W1_size    = 3  # 3 Neuronen in der 1.ten verborgenen Schicht\n",
    "W2_size    = 4  # 4 Neuronen in der 2.ten verborgenen Schicht\n",
    "W3_size    = 2  # 2 Ausgabeklassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7855277-9c9b-4a0a-92fe-fe0054e8a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[5],[7],[1]]) # Eingabewerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35eea6a-920f-4aa7-aaf4-2074dd7acf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere zufällige Gewichte und Biases\n",
    "W1 = np.random.rand(W1_size, input_size) - 0.5\n",
    "b1 = np.random.randn(input_size, 1)\n",
    "W2 = np.random.rand(W2_size, W1_size) - 0.5\n",
    "b2 = np.random.rand(W2_size, 1) - 0.5\n",
    "W3 = np.random.rand(W3_size, W2_size) - 0.5\n",
    "b3 = np.random.rand(W3_size, 1) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01e988-ecf6-45d3-a030-2e3e5ad51433",
   "metadata": {},
   "source": [
    "### Ausgabe versteckte Schicht $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b011e-241a-48ba-961f-141e21a69de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0cd0e-a76b-46f2-81b1-f6b1ce8101a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1eb8b-7aae-4bca-9a9e-e80c70183cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der ersten versteckten Schicht\n",
    "A1 = W1 @ X + b1\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d9850-2ae0-439f-a824-6c1b2b64f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape_inputs   :',X.shape, '\\n')\n",
    "print('shape_W1       :',W1.shape, '\\n')\n",
    "print('shape_b1       :',b1.shape, '\\n')\n",
    "print('shape_A1       :',A1.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c271fbf9-d65f-499d-b7b7-37a9145920ef",
   "metadata": {},
   "source": [
    "### Ausgabe versteckte Schicht $2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b467b-569a-40f7-8349-e5940759a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b445f-b127-4bbd-b763-4d19f2461233",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f8bfb-b554-447b-9315-88e949c83d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der zweiten versteckte schicht\n",
    "\n",
    "A2 = W2 @ A1 + b2\n",
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f77910-6c26-43a9-abc6-2cbe599b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape_A1       :',A1.shape, '\\n')\n",
    "print('shape_W2       :',W2.shape, '\\n')\n",
    "print('shape_b2       :',b2.shape, '\\n')\n",
    "print('shape_A2       :',A2.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128d878-4e96-4cf9-993f-d03dc23e3dbd",
   "metadata": {},
   "source": [
    "### Ausgabeschicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b3dad-6f82-434b-bb80-086e2675c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e5836-8759-4eb2-b198-a8aa973e50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8a4a2-b0f8-406e-ae18-9bc842a1a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabeschicht\n",
    "A3 = W3 @ A2 + b3\n",
    "A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41a05e-839a-4b00-a30f-5806e873ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape_A2       :',A2.shape, '\\n')\n",
    "print('shape_W3       :',W3.shape, '\\n')\n",
    "print('shape_b3       :',b3.shape, '\\n')\n",
    "print('shape_A3       :',A3.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3ed5b-fbf7-48a2-9c63-59dfba5e35f2",
   "metadata": {},
   "source": [
    "### Aktivierungsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2f941-9b19-418e-9389-5b47409410af",
   "metadata": {},
   "source": [
    "Gehen wir kurz auf eine weitere Feinheit des neuronalen Netzwerks ein, die Aktivierungsfunktion. Diese modulieren das weitergegebene Signal und sorgen für nichtlineare Modulation des Signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0baeae4-6360-450e-87ae-47fd294f95eb",
   "metadata": {},
   "source": [
    "Eine wesentliche Eigenschaft von Aktivierungsfunktionen ist, **nichtlineare Zusammenhänge** abbilden zu können. Würden wir nur die lineare gewichtete Summe der Eingaben verwenden, würden wir im Wesentlichen nur lineare Funktion miteinander verknüpfen, wodurch wieder lineare Funktionen entstehen. Betrachten wir zum Beispiel die zwei linearen Funktionen $f(g(x)) = 3 g(x) +1$ und $g(x) = 4 x +2$. Dann kann man die Verkettung dieser Funktionen $f(g(x))$ wie folgt schreiben:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1e787-fc2d-4dd4-aa52-02977586a2dc",
   "metadata": {},
   "source": [
    "$$ f(g(x)) = 3 g(x) + 1 = 3 (4 x + 2) + 1 = 12 x + 7 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a85e4-2a05-4733-8e8a-5c33c4b88e49",
   "metadata": {},
   "source": [
    "Das Ergebnis ist wieder eine lineare Funktion! Hingegen ist ein ausreichend großes künstliches neuronales Netz mit nicht linearen Aktivierungsfunktionen in der Lage, jede stetige Funktion zu approximieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cc506-e1a6-455f-bd04-c4056ee0635f",
   "metadata": {},
   "source": [
    "Eine häufig verwendete Aktivierungsfunktion ist in diesem Zusammenhang die aus der logistischen Regression bekannte **[Sigmoidfunktion](https://de.wikipedia.org/wiki/Sigmoidfunktion)**:\n",
    "\n",
    "$$A(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696adff7-ad30-44e0-a467-57b8c0cec092",
   "metadata": {},
   "source": [
    "Dabei ist $A(z)$ die Aktivierung des Neurons und $z = \\sum_i w_i x_i +b$ die gewichtete Summe der Inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d57520-4585-4c5e-9ef0-b4bb0409ecde",
   "metadata": {},
   "source": [
    "Die Sigmoidfunktion wird sowohl in versteckten Schichten als auch als Ausgabeaktivierungsfunktion in **binären Klassifikationsnetzwerken** eingesetzt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7fe37-d2e6-49a1-ac5f-2b8ba9df83c3",
   "metadata": {},
   "source": [
    "Eine der Sigmoidfunktion ähnliche Aktivierungsfunktion ist der **[Tangens hyperbolicus](https://de.wikipedia.org/wiki/Tangens_hyperbolicus_und_Kotangens_hyperbolicus)**:\n",
    "\n",
    "$$\\tanh (z) = \\frac{\\sinh (z)}{\\cosh (z)} = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd31e7-b912-48cc-bdcf-ee1d47983b2b",
   "metadata": {},
   "source": [
    "Beide Funktionen haben gemeinsam, den Wertebereich, auf den sie abbilden, einzuschränken. Im Fall der logistischen Funktion bildet diese beliebige Werte aus $\\mathbb{R}$ auf das Intervall $[ 0  \\ $ ,$ \\ 1 ]$. Beim des Tangens hyperbolicus bildet dieser Werte aus $\\mathbb{R}$ auf das Intervall $[ -1  \\ $ ,$ \\ 1 ]$ ab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93355b73-2323-44e6-987a-9e1b2e9d640c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Die Berechnungen der Sigmoid- als auch der Tangens-hyperbolicus-Funktion sind vergleichsweise rechenzeitintensive Operationen, da in ihnen die Terme $e^{\\pm z}$ berechnet und dividiert werden müssen. \n",
    "\n",
    "Als Alternative dient die **[Gleichrichterfunktion](https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke))** (*engl. rectified linear unit, ReLU*). Diese hat den Vorteil, dass sie einfacher als einige andere Aktivierungsfunktionen wie die Sigmoidfunktion oder die Tangens-hyperbolicus-Funktion zu berechnen ist. Sie trägt auch zur Vermeidung des Problems des verschwindenden Gradienten bei, das bei tiefen neuronalen Netzwerken auftreten kann. Die **ReLU-Funktion** ist wie folgt definiert:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7b9c6-67ab-4fa7-abed-185b342b6a28",
   "metadata": {},
   "source": [
    "$$ A(z) = max(0,z) \\begin{cases}\n",
    "z & \\text{für} \\ z \\gt 0, \\\\ 0 & \\text{sonst}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffbf6c-4ead-4e7f-ab85-df81177b1ea8",
   "metadata": {},
   "source": [
    "Obwohl für $z < 0$ bei den Werten der ReLU-Funktion keine Steigung existiert, zeigen ReLU-Aktivierungsfunktionen in künstlichen neuronalen Netzwerken eine sehr gute Optimierungsleistung und sind inzwischen eine der am häufigsten eingesetzten Aktivierungsfunktionen in tiefen neuralen Netzwerken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7682c-1530-42db-94be-710e637a3d94",
   "metadata": {},
   "source": [
    "Eine Erweiterung der ReLU-Funktion stellt die **[Leaky-ReLU-Funktion](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU)** dar. Die Idee dahinter ist, dass auch negative Werte für $z$ eine geringe Steigung aufweisen, um das sogenannte **[Vanishing-Gradient-Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)** zu umgehen. Die Leaky-ReLU-Funktion ist wie folgt definiert:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9c1f2-07a9-4624-9c17-296dce854b05",
   "metadata": {},
   "source": [
    "$$ A(z) \\begin{cases}\n",
    "z & \\text{für} \\ z \\gt 0, \\\\ 0,01 \\cdot z & \\text{sonst}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618ad76-1c3b-492f-b5e8-9bb374ec32d3",
   "metadata": {},
   "source": [
    "Eine weitere wichtige Aktivierungsfunktion für die **Ausgabeschicht von Klassifikationsnetzwerken** ist die **[Softmax-Funktion](https://de.wikipedia.org/wiki/Softmax-Funktion)**. Diese dient dazu, bei **Multiklassen-Klassifikation** die Wahrscheinlichkeitsverteilung der $K$ in unterschiedlichen möglichen Klassen zu berechnen. Die Softmax-Funktion ist in *Komponentenschreibweise* wie folgt definert:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b10901-6aa7-42a6-881d-2bbbd2ab7059",
   "metadata": {},
   "source": [
    "$$ A(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2e7ba-54d4-4d7c-bc02-b30640010c3d",
   "metadata": {},
   "source": [
    "Für drei Klassen wäre die **Softmax-Funktion** zum Beispiel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c918a0-fee1-48d6-a0d9-04e667f00d73",
   "metadata": {},
   "source": [
    "$$ A(z_1) = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}} \\ , \\   A(z_2) = \\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}} \\ , \\   A(z_3) = \\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d276fcd-4636-498b-a7c0-0aa184351f8f",
   "metadata": {},
   "source": [
    "Die Summe der einzelnen Komponenten addiert sich dabei im Sinne einer Wahrscheinlichkeit der Zugehörigkeit zu einer von $j$-Klassen zu $1$ auf:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ae2d4-71b7-4478-836a-debfe0d804b8",
   "metadata": {},
   "source": [
    "$$ A(z_1) +  A(z_2)+  A(z_3) = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}} + \\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}+ \\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}} =  1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230aa010-83d5-4f4d-8824-46b178d88f72",
   "metadata": {},
   "source": [
    "Im Unterschied zur Ausgabeaktivierung von Netzwerken zur Klassifikation wird in der **Ausgabeschicht von Regressionsnetzwerken** eine **lineare Aktivierungsfunktion** verwendet, um kontinuierliche Werte zu erhalten, die auf kein bestimmtes Intervall beschränkt sind. Die lineare Aktivierungsfunktion kann wie folgt geschrieben werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c06b1d-56b6-4a50-8f87-d0fd820fe6c0",
   "metadata": {},
   "source": [
    "$$A(z) = z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa15bec-259d-453b-8fe4-eb0e5e7fff51",
   "metadata": {},
   "source": [
    "In der folgenden Abbildung sind die wichtigsten **Aktivierungsfunktionen** nochmals zusammengefasst dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431be78-b094-4c2c-89ea-72d063ce6110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Werte generieren\n",
    "x = np.linspace(-6, 6, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "relu = np.maximum(0, x)\n",
    "tanh = np.tanh(x)\n",
    "linear = x\n",
    "softmax = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "leaky_relu = np.where(x > 0, x, 0.01 * x)  # Leaky ReLU mit Alpha = 0.01\n",
    "\n",
    "# Plot erstellen\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "\n",
    "# Sigmoid-Aktivierungsfunktion\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(x, sigmoid, label=\"Sigmoid\", color=\"b\")\n",
    "plt.xlabel(\"Eingabe\")\n",
    "plt.ylabel(\"Ausgabe\")\n",
    "plt.title(\"Sigmoid-Aktivierungsfunktion\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ReLU-Aktivierungsfunktion\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(x, relu, label=\"ReLU\", color=\"r\")\n",
    "plt.xlabel(\"Eingabe\")\n",
    "plt.ylabel(\"Ausgabe\")\n",
    "plt.title(\"ReLU-Aktivierungsfunktion\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Tanh-Aktivierungsfunktion\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(x, tanh, label=\"Tanh\", color=\"g\")\n",
    "plt.xlabel(\"Eingabe\")\n",
    "plt.ylabel(\"Ausgabe\")\n",
    "plt.title(\"Tanh-Aktivierungsfunktion\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Lineare Aktivierungsfunktion\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(x, linear, label=\"Lineare\", color=\"m\")\n",
    "plt.xlabel(\"Eingabe\")\n",
    "plt.ylabel(\"Ausgabe\")\n",
    "plt.title(\"Lineare Aktivierungsfunktion\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Softmax-Aktivierungsfunktion\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(x, softmax, label=\"Softmax\", color=\"c\")\n",
    "plt.xlabel(\"Eingabe\")\n",
    "plt.ylabel(\"Ausgabe\")\n",
    "plt.title(\"Softmax-Aktivierungsfunktion\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Leaky ReLU-Aktivierungsfunktion\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(x, leaky_relu, label=\"Leaky ReLU\", color=\"y\")\n",
    "plt.xlabel(\"Eingabe\")\n",
    "plt.ylabel(\"Ausgabe\")\n",
    "plt.title(\"Leaky ReLU-Aktivierungsfunktion (Alpha=0.01)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f4550-ac92-490c-8f67-2c84722e7c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Einfaches neuronales Netzwerk mit Aktivierungsfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb68a6-778e-46a2-87b8-bb9b98ad6971",
   "metadata": {},
   "source": [
    "Wenden wir Aktivierungsfunktionen auf das oben skizzierte Netzwerk an. Dabei verwenden wir in dersten und zweiten versteckten Schicht **ReLU-Aktivierung** und in der Ausgabeschicht **Sigmoid-Aktivierung**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8a2f9-07b1-4ed7-a8fa-628fb0e4b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28031abc-6168-487b-a2b3-2e89266fa9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return (1/(1 + np.exp(1)**(-X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3df564-5fa2-4341-8b45-00eb732dfd5e",
   "metadata": {},
   "source": [
    "### Feedforward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcb3b1-3cbd-4bde-b232-6cfc5000e881",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialisieren der Gewichte und Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64166483-2e7a-479e-a1c4-ffd08141a66f",
   "metadata": {},
   "source": [
    "Wir versuchen aus den bisherigen theoretischen Überlegungen ein neuronales Netzwerk zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940bb32-974e-4c1b-9505-818a21d8cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3  # Eingabeschicht\n",
    "W1_size    = 3  # 3 Neuronen in der 1.ten verborgenen Schicht - ReLU-Aktivierung\n",
    "W2_size    = 4  # 4 Neuronen in der 2.ten verborgenen Schicht - ReLU-Aktivierung\n",
    "W3_size    = 2  # 2 Ausgabeklassen - Sigmoidaktivierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6d341-8bab-4337-a285-3e5e72e00502",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([[5],[7],[1]]) # Eingabewerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c67af8-b59d-429b-9cee-cc8030570014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere zufällige Gewichte und Biases\n",
    "W1 = np.random.rand(W1_size, input_size) - 0.5\n",
    "b1 = np.random.randn(input_size, 1)\n",
    "W2 = np.random.rand(W2_size, W1_size) - 0.5\n",
    "b2 = np.random.rand(W2_size, 1) - 0.5\n",
    "W3 = np.random.rand(W3_size, W2_size) - 0.5\n",
    "b3 = np.random.rand(W3_size, 1) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921436d0-524b-4cbf-ae03-76dee0fce851",
   "metadata": {},
   "source": [
    "### Ausgabe versteckte Schicht $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87667380-6208-4d12-9971-f1709097f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fe794-e858-43f8-97af-0fad70362596",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fb4d3-55f5-4303-a476-eb5fb43a3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der ersten versteckten Schicht mit ReLU-Aktivierung\n",
    "A1 = relu_func(W1 @ X + b1)\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db4243-92c4-48f5-8990-b7741f7c69c0",
   "metadata": {},
   "source": [
    "### Ausgabe versteckte Schicht $2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c34cb-76f3-42b9-b3fa-5b92b30322b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddf341-5916-4165-a18f-398c44a2441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8c74a-dfb7-46fe-bb92-9a732543c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der zweiten versteckte schicht mit ReLU-Aktivierung\n",
    "A2 = relu_func(W2 @ A1 + b2)\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4f4ea-1279-49ff-ad9b-5836a35f85c4",
   "metadata": {},
   "source": [
    "### Ausgabeschicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9081536-f30d-4015-b50c-9ebc0c3254e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e58df-096a-4dda-8251-6e652f86951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127ccff-1652-4478-80e3-41367966c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabeschicht mit Sigmoidaktivierung\n",
    "A3 = sigmoid(W3 @ A2 + b3)\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95379af-f6b0-46f5-897b-f726be789115",
   "metadata": {},
   "source": [
    "## Numerische Ableitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a4c21-ba14-441f-8e99-48b227ba9f3a",
   "metadata": {},
   "source": [
    "In Vorbereitung auf den **Backpropagation-Algorithmus** beschäftigen wir uns kurz mit numerischer Ableitung von Funktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c92d61e-1f6d-4482-b5b9-775e95157827",
   "metadata": {},
   "source": [
    "Sehen wir uns als Beispiel die Funktion $f(x) = x^5 + x^3 + x$ und ihre Ableitung an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc339ea-9450-41e0-91fb-1cfbab3be1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**5 + x**3 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2432f-1594-4639-808c-b5d58b8d72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abbildung f(x) = x**5 + x**3 + x\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "x = np.linspace(-2,2, 1000)\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "_ = plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40e1b5-3f8a-4210-8137-a73d669b4090",
   "metadata": {},
   "source": [
    "### Differenzenquotient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba2672-05c9-4dde-b78b-d9535ac4ea8a",
   "metadata": {},
   "source": [
    "Um die Funktion numerisch abzuleiten können wir als einfachste Annäherung den **Differenzenquotienten** $\\frac{ f (x + \\epsilon) - f (x)}{(x + \\epsilon) - x }$ der Funktion an den Stellen $x$ und $x + \\epsilon$ verwenden, wobei $\\epsilon$ die Schrittweite bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40d661-002c-413e-a325-45acafab0e5f",
   "metadata": {},
   "source": [
    "$$f^{\\prime}(x) = \\frac{d f (x)}{d x}  \\approx \\frac{ f (x + \\epsilon) - f (x)}{(x + \\epsilon) - x } = \\frac{ f (x + \\epsilon) - f (x)}{\\epsilon} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1d011-b891-4cbf-b16c-a45e9cff605c",
   "metadata": {},
   "source": [
    "$$eg.: f^{\\prime}(x) = (x^2)^{\\prime} = 2x  \\approx \\frac{ (x + \\epsilon)^2 - (x)^2}{\\epsilon} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf0abd-f9ef-4bad-9dd6-3ec3d0be87c0",
   "metadata": {},
   "source": [
    "Wir können dies in `Python` folgendermaßen anschreiben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb4e8a-ff78-4c60-93ab-33365db49346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(f, x, delta):\n",
    "    return (f(x + delta) - f(x))/delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f75192-9aa1-4be9-afe4-7e5771dc8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abbildung f(x), f'(x)\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.grid()\n",
    "plt.xlim([-2.2,2.2])\n",
    "plt.ylim([-2.2,7.2])\n",
    "plt.yticks(np.arange(-2, 8, 1))\n",
    "plt.xticks(np.arange(-2, 3, 1))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.plot(x,y)\n",
    "_ = plt.plot(x, derivative(f, x, 10**-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ff636-efa8-46ab-91be-ec0dab06f0fc",
   "metadata": {},
   "source": [
    "### Ableitung der ReLU-Aktivierungsfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6953e-a687-443d-9fee-c6ff46875387",
   "metadata": {},
   "source": [
    "Die ReLUfunktion ist gegeben durch $f_{ReLU}(x) = max(0, x)$. Versuchen wir diese wichtige und vielleicht etwas unintuitive Funktion mit unserer Funktion `derivative()` numerisch abzuleiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff88da-0e88-4e51-8813-4291fd201717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9958c-4492-48a5-b363-6c70d86bc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(relu_func, x, delta = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e52a0a-a27d-4faa-8ed6-39e78805843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.plot(x, derivative(relu_func, x, delta = 1e-5), linewidth = 2.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"H(x)\")\n",
    "plt.title(\"Heaviside-Sprungfunktion als Ableitung der ReLU-Funktion\")\n",
    "plt.ylim([-0.5,1.5])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f9554-3577-40ab-9a62-4fb4e6c90d50",
   "metadata": {},
   "source": [
    "### Problem der numerischen Ableitung - unstetige Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de6041-b192-4515-968f-a0537e007621",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abbildung ReLU-Funktion und Ableitung der ReLU-Funktion\n",
    "# ReLU-Funktion und numerische Ableitung\n",
    "def relu_func(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative(func, x, delta=1e-5):\n",
    "    return (func(x + delta) - func(x)) / (delta)\n",
    "\n",
    "# Funktion, die den Plot aktualisiert\n",
    "def plot_reAct(num_points):\n",
    "    x = np.linspace(-2, 2, num_points)  # Anzahl der Punkte anpassen\n",
    "\n",
    "    plt.figure(dpi=600, figsize=(6, 3))\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ReLU-Funktion\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, relu_func(x), label='ReLU(x)', color='blue')\n",
    "    plt.title('ReLU-Funktion')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('ReLU(x)')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Numerische Ableitung der ReLU-Funktion\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, derivative(relu_func, x, delta=1e-5), label=\"ReLU'(x)\", color='red')\n",
    "    plt.title('Numerische Ableitung der ReLU-Funktion')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(\"ReLU'(x)\")\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Schieberegler für die Anzahl der dargestellten Punkte\n",
    "interactive_plot = interactive(plot_reAct, num_points=(70, 1000, 1))  # Schieberegler von 70 bis 1000 Punkten\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a35dff-d3d6-4cdf-96c0-ea9940459d6d",
   "metadata": {},
   "source": [
    "Eine einfachere Lösung besteht darin nicht die tatsächliche Ableitung zu bilden, sondern die Ableitung über die Heaviside-Funktion festzulegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77128da-8c05-45b8-ba23-d70f43f873c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_ReLU(Z):\n",
    "    return np.where(Z > 0, 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd646e-c086-4cb5-9cf9-e4cbb23f7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2, 1000)\n",
    "plt.title('Die Heaviside-Funktion als Ableitung der ReLU-Funktion')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"ReLU'(x)\")\n",
    "_ = plt.plot(x, derivative_ReLU(x), color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba2730-c0d4-4925-bb86-a826784ebb4b",
   "metadata": {},
   "source": [
    "## Gradientenabstiegsverfahren (engl. Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e72085-b05e-4fdf-8c9c-03ba10ac8f63",
   "metadata": {},
   "source": [
    "Beim klassischen **[Gradientenverfahren](https://de.wikipedia.org/wiki/Gradientenverfahren)** geht es darum, im Idealfall ein globales Maximum/Minimum einer gegebenen Funktion zu bestimmen, man spricht auch von einem sogenannten **[Optimierungsproblem](https://de.wikipedia.org/wiki/Optimierung_(Mathematik))**. Die Vorgehensweise ist dabei die Bildung des namensgebenden **[Gradienten](https://de.wikipedia.org/wiki/Gradient)** einer Funktion für alle unabhängigen Parameter. Dabei zeigt der Gradient in die Richtung des größten Anstiegs der Funktion. Um zum Minimum der Funktion zu kommen, wählen wir einen Startwert $w_{\\text{alt}}$, der als Ausgangspunkt des iterativen Abstiegs zum Minimum dient, und gehen bei jedem Schritt des Gradientenverfahrens in Richtung des negativen Gradienten der Funktion, indem wir den Gradienten mal einer Lernrate $\\alpha$ von $w_{\\text{alt}}$ abziehen, um $w_{\\text{neu}}$ zu berechnen. Allgemein kann man schreiben:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff2c9e-59e6-4751-8c54-ab91d5ac2b9b",
   "metadata": {},
   "source": [
    "$$w_{\\text{neu}} = w_{\\text{alt}} - \\alpha \\cdot \\nabla f(w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec977c1-d24a-42e3-94ca-d61f24dafe1e",
   "metadata": {},
   "source": [
    "Dabei ist $\\nabla$ der **[Nabla-Operator](https://de.wikipedia.org/wiki/Nabla-Operator)**. Im eindimensionalen Fall entspricht er einfach der Ableitung nach der unabhängigen Variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b33090-2f6e-46a5-9b7a-ff84e0c9c326",
   "metadata": {},
   "source": [
    "Betrachen wir zum Beispiel, das Minimum der Funktion $f(x)= x^2$ zu bestimmen, um das Gradientenabstiegsverfahren zu verstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de505137-9953-4de5-8a30-22c3cb962e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "x = np.linspace(-5, 5, N)\n",
    "y = x**2\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c0caa-9da3-40e7-9fc7-acb9657dc573",
   "metadata": {},
   "source": [
    "Wir bestimmen den Gradienten der Funktion $f(x)=x^2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddf4e6-4414-4629-a366-03dcaf4b55de",
   "metadata": {},
   "source": [
    "$$\\frac{df}{dx} = 2 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31781d4-888b-4aea-b464-50bbd3dc2a64",
   "metadata": {},
   "source": [
    "Um das Minimum zu bestimmen, subtrahieren wir den Gradienten multipliziert mit einer Lernrate $\\alpha$ von einem zufällig gewählten Startwert und iterieren so lange, bis der Gradient gegen $0$ konvergiert. Wir gehen somit in die Gegenrichtung des größten Zuwachses der Funktion mit jeder Iteration auf ein Minimum zu. In jedem Schritt berechnen wir den nächsten $x$-Wert mit:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04046595-6eb5-4fe7-bb62-c897cd03dabb",
   "metadata": {},
   "source": [
    "$$x_{\\text{neu}} = x_{\\text{alt}} - \\alpha \\cdot \\frac{df}{dx} = x_{\\text{alt}} - \\alpha \\cdot 2 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6216a-0ade-4e20-977a-b416b04277db",
   "metadata": {},
   "source": [
    "Sehen wir uns dazu ein Code-Beispiel an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b870027-980a-42df-8cb8-952431e44f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_alt = 5\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff856299-9298-4dcc-8ace-a6ffe7d7d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 30):\n",
    "    x_neu = x_alt - alpha * (2 * x_alt)\n",
    "    x_alt = x_neu\n",
    "x_neu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cdf775-2581-4e54-a6c4-ebe9e650a15e",
   "metadata": {},
   "source": [
    "Wie wir sehen können, konvergiert der Wert für $x_{\\text{neu}}$ gegen das Minimum von $f(x)$ bei $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5e211-d193-40a9-8e03-504d84e02326",
   "metadata": {},
   "source": [
    "In der folgenden Abbildung sehen Sie die ersten drei Schritte beim Gradientenabstiegsverfahren mit Lernrate `alpha = 0.1` und ausgehend vom Startpunkt `x_alt = 5`. Beachten Sie, dass die Schrittweite abnimmt, je näher wir dem Minimum kommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71130666-848b-4994-8750-549b04935ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zielfunktion und deren Ableitung\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "# Startpunkt und Lernrate für den Gradientenabstieg\n",
    "x_start = 5.0\n",
    "alpha = 0.1\n",
    "\n",
    "# Anzahl der Schritte\n",
    "num_steps = 3\n",
    "x_history = [x_start]\n",
    "\n",
    "# Gradientenabstieg durchführen und die Pfeile zeichnen\n",
    "for _ in range(num_steps):\n",
    "    x_current = x_start\n",
    "    x_start = x_start - alpha * df(x_start)\n",
    "    x_history.append(x_start)\n",
    "\n",
    "# X-Werte für die Funktion\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Diagramm erstellen\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.plot(x, f(x), label=\"f(x) = $x^2$\", color=\"blue\")\n",
    "_ = plt.scatter(\n",
    "    x_history, [f(x) for x in x_history], color=\"red\", label=\"Gradient Descent Steps\"\n",
    ")\n",
    "\n",
    "# Pfeile zeichnen, die die Schritte des Gradientenabstiegs verbinden\n",
    "for i in range(1, len(x_history)):\n",
    "    dx = x_history[i] - x_history[i - 1]\n",
    "    dy = f(x_history[i]) - f(x_history[i - 1])\n",
    "    plt.quiver(\n",
    "        x_history[i - 1],\n",
    "        f(x_history[i - 1]),\n",
    "        dx,\n",
    "        dy,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"green\",\n",
    "        width=0.0075,\n",
    "        headaxislength=4,\n",
    "        headlength=4,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient Descent\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8e922-9f8f-4fcd-950f-dbfd35aa84c1",
   "metadata": {},
   "source": [
    "Die Konvergenzgeschwindigkeit (die notwendige Anzahl an Schritten) ist dabei von der Lernrate und dem zufällig gewählten Startpunkts abhängig. Dabei gilt es, einen Mittelweg zwischen einer **zu kleinen Lernrate**, die zu einer unnötig **hohen Anzahl von Schritten** führt, und einer **zu hohen Lernrate**, die zu **oszillierenden Lösungen** führt, da sie immer wieder über das Minimum hinweg springt, zu finden. Oft ist das Finden der besten Lernrate ein iterativer Prozess des Experimentierens. Sie können mit verschiedenen Lernrateneinstellungen beginnen (z.B. $0.1$, $0.01$, $0.001$) und die Leistung auf einem Validierungsdatensatz überwachen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354b978-58b2-46f4-b505-96be4a3b1e3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Versuchen wir jetzt das Gradientenabstiegsverfahren anzuwenden um ein neuronales Netz zu trainieren und betrachten dazu das folgende Beispiel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfdd01-604e-41c0-9b6b-36e2be70af36",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e814fd-1102-465d-b18e-f6fc61955c14",
   "metadata": {},
   "source": [
    "Sehen wir uns ein Beispiel für den **Backpropagation-Algorithmus** Schritt für Schritt an. Aus Gründen der Übersichtlichkeit verwenden wir dabei ein Netz mit zwei Eingaben, einer versteckten Schicht mit zwei Neuronen und **linearer Aktivierung** und einer Ausgabeschicht mit **Sigmoid-Aktivierung**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50a6c8-5268-4d33-a02e-e7233c9933b0",
   "metadata": {},
   "source": [
    "## Forwardpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383fe77-c4d6-45ab-9e66-2e712332dba7",
   "metadata": {},
   "source": [
    "Zuerst führen wir für das in der Abbildung gezeigte neuronale Netz wie gewohnt eine Forwardpropagation durch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f0e9e-dcf9-466d-9ceb-bb9a36998076",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_forward.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ce6ca1-5a65-49d2-9e82-c6afe2e8983c",
   "metadata": {},
   "source": [
    "### Anfangswerte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d32c6-34b0-4e83-9cab-27c26a620cf7",
   "metadata": {},
   "source": [
    "Wir gehen von folgenden Ausgangsparametern für Gewichte und Biases aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d3e24-733b-44c4-b443-7ec4d54ec794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X1 = 0.85\n",
    "\n",
    "X2 = 0.5\n",
    "\n",
    "w1 = 0.75\n",
    "\n",
    "w2 = 0.55\n",
    "\n",
    "w3 = 0.05\n",
    "\n",
    "w4 = 0.05\n",
    "\n",
    "w5 = 0.05\n",
    "\n",
    "w6 = 0.015\n",
    "\n",
    "w7 = 0.85\n",
    "\n",
    "w8 = 0.95\n",
    "\n",
    "b1 = 0.25\n",
    "\n",
    "b2 = 0.15\n",
    "\n",
    "b3 = 0.15\n",
    "\n",
    "b4 = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f791fdb-8666-420e-8a5b-44b34686f994",
   "metadata": {},
   "source": [
    "Wie verwenden für die Eingabe- und die versteckte Schicht lineare Aktivierung und für die Ausgabeschicht die Sigmoidaktivierungsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143728f-1015-4d57-9566-a1173905d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return (1/(1 + np.exp(1)**(-X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0a5ef-5283-4d3f-ac79-98169b75ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764ad7f-d594-4b2e-bcd6-186e58c78e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), dpi = 600)\n",
    "x_werte = np.linspace(-20,20,1000)\n",
    "ax.set_title('Sigmoidfunktion', fontsize = 14)\n",
    "_ = ax.plot(x_werte, sigmoid(x_werte))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48223662-c8ac-41e1-b7f3-7f5003eded7d",
   "metadata": {},
   "source": [
    "Die Ableitung der Sigmoidfunktion ist gegeben durch:\n",
    "\n",
    "$S(x)^{\\prime} = S(x) (1 - S(x)) $\n",
    "\n",
    "Wir können dies überprüfen, indem wir mit unserer Funktion `derivative()` die Sigmoidfunktion ableiten und das Ergebnis gleichzeitig mit dem oberen Ausdruck plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6feefff-e349-41a2-b334-707cc431ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), dpi = 600)\n",
    "ax.set_title('Ableitung der Sigmoidfunktion', fontsize = 14)\n",
    "#plt.plot(x_werte, sigmoid(x_werte)*(1 - sigmoid(x_werte)))\n",
    "_ = ax.plot(x_werte, derivative(sigmoid, x_werte, 10**-12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e6599-999c-4e50-8b20-a2c0bae779e8",
   "metadata": {},
   "source": [
    "Wir berechnen den Vorwärtsdurchlauf durch das Netz bis zu den Aktivierungen $A_3$, $A_4$ in der Ausgabeschicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfccb1c-8395-416d-9e42-81169f351366",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = w1 * X1 + w2 * X2 + b1\n",
    "Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad4d64-5c4c-499b-a076-8a95bf4d36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = w3 * X1 + w4 * X2 + b2\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bb6a9-55cb-481e-ad01-af421dfe868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = linear_activation(Z1)\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc6d21-7748-483e-8aef-d534fc099ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = linear_activation(Z2)\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7685843-f120-40a5-bdc3-9428108f2eee",
   "metadata": {},
   "source": [
    "#### Aktivierung im Ausgabeneuron $1$: $A_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807534f-f320-4d51-b8d8-f9dab580e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z3 = w5 * A1 + w6 * A2 + b3\n",
    "Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5879c49-3c73-48f6-9520-f3971e0d7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = sigmoid(Z3)\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802d816-214c-492f-bbee-011c0534e300",
   "metadata": {},
   "source": [
    "#### Aktivierung im Ausgabeneuron $2$: $A_4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26381e0-ef43-4f2e-99d5-d4430312c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z4 = w7 * A1 + w8 * A2 + b4\n",
    "Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a5ebe-d232-4c03-a27f-72b3dbc94aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = sigmoid(Z4)\n",
    "A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d7150-89c8-4cac-add9-744bb1046132",
   "metadata": {},
   "source": [
    "### Verlustfunction - Mean Squared Error (MSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf78016-21e8-43e5-a4fd-be16a8f29e50",
   "metadata": {},
   "source": [
    "Um den Fehler der Vorhersage zu dem tatsächlichen Wert der Ausgabe zu bestimmen verwenden wir den "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e18812-c14f-4e50-ac19-5c8dd0a72a21",
   "metadata": {},
   "source": [
    "Zuerst müssen wir ein Maß für den Fehler des Modells einführen. Ein Möglichkeit den Fehler eines Modells zu bewerten ist durch den **[MSE (Mean Squared Error)](https://de.wikipedia.org/wiki/Mittlere_quadratische_Abweichung)** gegeben. Allgemein spricht man von **[Verlustfunktionen](https://de.wikipedia.org/wiki/Verlustfunktion_(Statistik))** (*engl. loss function*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ea74b-532c-41f4-b445-87334883d151",
   "metadata": {},
   "source": [
    "$$ MSE = E_{total} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98915acf-8665-45cb-b72e-8aa4834c883a",
   "metadata": {},
   "source": [
    "Dabei ist $y_i$ die erwartete Ausgabe (*engl. ground truth*) und $\\hat y_i$ die Vorhersage des Modells (*engl. prediction*). Wir nehmen an $\\hat y_1 = 0.01$ und $\\hat y_1 = 0.99$ gegeben sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea2f4f-6545-4ca8-9d50-1b2a19a0c840",
   "metadata": {},
   "source": [
    "Der Index $n$ läuft dabei über alle Neuronen der Ausgabeschicht. In unserem Beispiel gilt also $E_{total} = E_1 + E_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df249232-084f-465d-b111-8af64402cce3",
   "metadata": {},
   "source": [
    "$E_1 = \\frac{1}{2} (\\hat y_1 - A_3 )^2 = \\frac{1}{2} (0.01 - A_3 )^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a3ca2f-35a0-4e30-b0f8-09e2a1e27ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehler Ausgabeschicht - erstes Neuron\n",
    "E_1 = 1/2*(0.01 - A3)**2 \n",
    "E_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2ab34-b790-437f-8c66-33933fcee3ea",
   "metadata": {},
   "source": [
    "$E_2 = \\frac{1}{2} (\\hat y_2 - A_4 )^2 = \\frac{1}{2} (0.99 - A_4)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eba6f7-d913-4d1c-b2b5-bee3e456cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehler Ausgabeschicht - zweites Neuron\n",
    "E_2 = 1/2*(0.99 - A4)**2 \n",
    "E_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43506d29-ac23-4d99-be0f-90bdd00e50c2",
   "metadata": {},
   "source": [
    "$E_{total} = E_1 + E_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcb661-da2a-47b7-8a7f-275e6c2fdf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_total = E_1 + E_2\n",
    "E_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b0fe0-6a49-460c-90d5-582d7fd96017",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d13cdd-d1c4-4446-81ae-de0ea85fd5e1",
   "metadata": {},
   "source": [
    "Wir haben also den Gesamtfehler der Ausgabe durch $E_{total}$ gegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96c5fe-a49e-4f25-b856-1c1ef84eb3b1",
   "metadata": {
    "citation-manager": {
     "citations": {
      "0t31g": [
       {
        "id": "16738657/H9BKTPET",
        "source": "zotero"
       }
      ],
      "c4fkk": [
       {
        "id": "16738657/P59K4ZW6",
        "source": "zotero"
       }
      ],
      "k1ov4": [
       {
        "id": "16738657/GXMJ3Q3D",
        "source": "zotero"
       }
      ],
      "kbsyk": [
       {
        "id": "16738657/ZZNR2L5E",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "Wenden wir uns jetzt dem Optimierungsverfahren dieses Fehlers, dem **[Backpropagation-Algorithmus](https://de.wikipedia.org/wiki/Backpropagation)**, zu. Dafür greifen wir auf die von uns zuvor besprochenen Grundlagen wie das **Gradientenverfahren** und **Aktivierungsfunktionen** zurück und versuchen systematisch die Gewichte des Netzwerks so anzupassen um bessere Vorhersagen zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95443a-f410-4b5f-bd0f-585c62e0fbb4",
   "metadata": {},
   "source": [
    "Dabei gehen wir von den Ausgaben des Neuronalen Netzes rückwärts und passen jeweils die Gewichte und Biases in Richtung des Gradientenabstiegsverfahrens an. Um die Abhängigkeiten des Fehlers (der Verlustfunktion) zu berechnen müssen wir dabei die Kettenregel anwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd921a9c-d5a6-4648-ab7a-d39a5af2bd35",
   "metadata": {},
   "source": [
    "Zur Erinnerung sehen wir uns die Anwendung der Kettenregel an einem Beispiel an:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43df180-5a1f-4174-a40f-b4c6e64edce4",
   "metadata": {},
   "source": [
    "### Kettenregel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1a7c8-ad47-480d-aa1f-fd3cdf04e88a",
   "metadata": {},
   "source": [
    "$f(g(x))^{\\prime} = \\frac{df}{dg}\\frac{dg}{dx}$\n",
    "\n",
    "e.g.:\n",
    "\n",
    "$f = g^2, g = sin(x)$\n",
    "\n",
    "$\\frac{d}{dx}(sin(x))^2 = 2 \\cdot sin(x) \\cdot cos(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d688c3e-5a04-4a91-816d-ca256cf454e9",
   "metadata": {},
   "source": [
    "## Ausgabeschicht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f463e3f-13c9-4c83-9e31-9f0ebf9f9bda",
   "metadata": {},
   "source": [
    "Berechenen wir ausgehend von der Ausgabeschicht di Anpassung der Gewichte ($w_5, w_6, w_7, w_8$) der Ausgabeschicht: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf16479-a8d0-4961-83f1-038fa0113560",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w5_f2.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20639780-0058-4cfd-acd4-f4dd99776d46",
   "metadata": {},
   "source": [
    "### Änderung von $E_{total}$ nach $w_5$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcaaabd-ce56-48e0-81a5-56469123834c",
   "metadata": {},
   "source": [
    "#### Gesamte Ableitung: $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_5}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b68a03-6470-433f-8ad8-1491a4f48d40",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165bb68-e224-4129-8c66-53a89d7bd67a",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_3} = \\frac{\\partial }{\\partial A_3}(E_1 + E_2 ) = \\frac{\\partial }{\\partial A_3}(\\frac{1}{2} (\\hat y_1 - A_3 )^2 + \\frac{1}{2} (\\hat y_2 - A_4 )^2 ) = \\frac{\\partial }{\\partial A_3}(\\frac{1}{2} ( 0.01 - A_3 )^2 + \\frac{1}{2} (0.99 - A_4 )^2 )  \\\\ = -(0.01 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec53751-750c-4008-afde-7d6a53e25295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_A3 = -(0.01 - A3)\n",
    "dE_total_nach_A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23020c43-09fe-42ee-8806-e776fdbc1da4",
   "metadata": {},
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9779d-0aaa-4260-9241-1c3abf6a86b6",
   "metadata": {},
   "source": [
    "####  $\\frac{\\partial A_3}{\\partial Z_3} = -\\frac{\\partial }{\\partial Z_3} \\frac{1}{1 + e^{-Z_3}} = A_3 (1 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e891db-d63b-4303-ac88-35f2c1db4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA3_nach_Z3 = A3 * (1 - A3)\n",
    "dA3_nach_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be6691-fcd2-421c-8e0b-20234c430b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf337d-b65c-4c2c-93e9-9c300241fad4",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_3}{\\partial w_5} = \\frac{\\partial }{\\partial w_5}(w_5 \\cdot A_1 + w_6 \\cdot A_2 + b_3) = A_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908e641-5e97-42d2-b87c-babe9ed45594",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_nach_w5 = A1\n",
    "dZ3_nach_w5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba429d7-dfdd-4e15-8b78-6f2ff7779949",
   "metadata": {},
   "source": [
    "#### Gesamte Ableitung: $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_5}   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b5ede-5c3a-4f91-8082-83207952595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w5 = dE_total_nach_A3 * dA3_nach_Z3 * dZ3_nach_w5\n",
    "dE_total_nach_w5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250239b-a654-401a-97bc-ab6fc2f6241a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_5$ anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdcac4-dfbc-42d2-a60f-5e445ac13cee",
   "metadata": {},
   "source": [
    "$w_{5 neu} = w_5 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_5}$, $\\alpha = 0.5 \\cdots \\text{Learning rate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166e0f1-c34d-4dd4-95d9-d913373425d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w5_neu = w5 - 0.5 * dE_total_nach_w5\n",
    "w5_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c709c9-e7f8-4a5f-92f7-78cc1c5c725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w5\n",
    "delta_w5 = w5_neu - w5\n",
    "delta_w5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bf5ef-7c52-4e41-8211-a987bca71cd0",
   "metadata": {},
   "source": [
    "### Änderung von $E_{total}$ nach $w_6$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f9915-8146-4536-b81e-538f531552c8",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w6_f3.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc1a56-de60-4085-9639-1ade82599210",
   "metadata": {},
   "source": [
    "#### Gesamte Ableitung: $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_6}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7726300-84c3-4197-8647-9f6495118e35",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959a6a6-3caa-47f8-b099-3a5c8d059d00",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_3} = -(0.01 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eacbc0-561b-4b32-9c98-d5ee6f997469",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_A3 = -(0.01 - A3)\n",
    "dE_total_nach_A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a708f9-0244-4fa8-8cc5-48d7892de691",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e921877-95e5-49df-8b69-86343c819545",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  $\\frac{\\partial A_3}{\\partial Z_3} = -\\frac{\\partial }{\\partial Z_3} \\frac{1}{1 + e^{-Z_3}} = A_3 (1 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31589585-cafd-4a98-bec9-f0305c6b22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA3_nach_Z3 = A3 * (1 - A3)\n",
    "dA3_nach_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be87911-83bc-4621-971b-e7f84a3cebda",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15d9b8-7e02-430e-908e-77bba36df5d3",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_3}{\\partial w_6} = \\frac{\\partial }{\\partial w_6}(w_5 \\cdot A_1 + w_6 \\cdot A_2 + b_3) = A_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e4864-ced8-4a76-aee0-5a686b401c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_nach_w6 = A2\n",
    "dZ3_nach_w6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5a6d0-f506-4e9c-a711-8dce5ef65e15",
   "metadata": {},
   "source": [
    "#### Gesamte Ableitung: $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_6}   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0e937-1c7b-4dc1-b8eb-cd2e2c93215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w6 = dE_total_nach_A3 * dA3_nach_Z3 * dZ3_nach_w6\n",
    "dE_total_nach_w6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6734f-3ed0-43c2-8fcb-9e516170fc29",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_6$ anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a88c7d-ff11-49c5-a126-58cf18507b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "w6_neu = w6 - 0.5 * dE_total_nach_w6\n",
    "w6_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816dce87-8160-4106-ae11-6538363000ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w6\n",
    "delta_w6 = w6_neu - w6\n",
    "delta_w6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3892d6-c9a6-4bae-903b-7c5d934b3f76",
   "metadata": {},
   "source": [
    "### Änderung von $E_{total}$ nach $w_7$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99766b3-8c51-4ae7-a775-7dd66bced484",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w7_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9cffb8-e2cd-4cc1-b9c2-30b9ad42bbfa",
   "metadata": {},
   "source": [
    "#### Gesamte Ableitung: $\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial w_7}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987068f-20c9-449c-8e12-0e576458e279",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37c56f-246f-4e8d-930e-eb935e5107c5",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_4}  =\\frac{\\partial }{\\partial A_4}(\\frac{1}{2} (\\hat y_1 - A_3 )^2 + \\frac{1}{2} (\\hat y_2 - A_4 )^2 ) \\\\ = \\frac{\\partial }{\\partial A_4}(\\frac{1}{2} ( 0.01 - A_3 )^2 + \\frac{1}{2} (0.99 - A_4 )^2 )  \\\\ = -(0.99 - A_4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af32ed8-ad1a-4cd9-9bcd-4ef591b534ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_A4 = -(0.99 - A4)\n",
    "dE_total_nach_A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d7041-d4f6-4693-a122-6b9aed101afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed62d41-0e28-4978-b4e2-264dac1fdd46",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  $\\frac{\\partial A_4}{\\partial Z_4} = \\frac{\\partial }{\\partial Z_4} \\frac{1}{1 + e^{-Z_4}} = A_4 (1 - A_4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f57f3b-f5b2-43b6-a4f1-78b7af117787",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA4_nach_Z4 = A4 * (1 - A4)\n",
    "dA4_nach_Z4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7975f-dd09-4d36-b296-afa1831f186e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927dd990-253b-4ea9-a8eb-9dbeb388654e",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_4}{\\partial w_7} = \\frac{\\partial }{\\partial w_7}(w_7 \\cdot A_1 + w_8 \\cdot A_2 + b_4) = A_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79052a13-09f4-4673-8f54-a2b3574c16b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dZ4_nach_w7 = A1\n",
    "dZ4_nach_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d368da-72a9-4a26-9998-46e524f071a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w7 = dE_total_nach_A4 * dA4_nach_Z4 * dZ4_nach_w7\n",
    "dE_total_nach_w7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424978c1-af92-419c-ae45-885960f9a879",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_7$ anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5a486-42f2-464a-9de0-05885f078e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w7_neu = w7 - 0.5 * dE_total_nach_w7\n",
    "w7_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3859d1-58c0-4144-a4eb-e98a92ebaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w7\n",
    "delta_w7 = w7_neu - w7\n",
    "delta_w7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7e021-dee6-4a56-b23a-588ce57a16c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Änderung von $E_{total}$ nach $w_8$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70642f16-d3a6-4c06-96b6-5b3a0ddf6b97",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w8_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51821e-97aa-4101-be3c-874f61ecb551",
   "metadata": {},
   "source": [
    "#### Gesamte Ableitung: $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial w_8}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c80ce3-aa26-4185-ab73-9211e164420e",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9c8ff-1c6f-4f97-8f96-044441f89c6c",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_4} = -(0.99 - A_4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b860056-fdb1-4e6a-bcaf-8b4fddce88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_A4 = -(0.99 - A4)\n",
    "dE_total_nach_A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d0436-31fe-49f9-89d8-14017b4b7242",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7128d-7f03-44a3-99ae-afa0811d7ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  $\\frac{\\partial A_4}{\\partial Z_4} = \\frac{\\partial }{\\partial Z_4} \\frac{1}{1 + e^{-Z_4}} = A_4 (1 - A_4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0aa35d-c8d7-4cd9-ae70-034c4c107146",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA4_nach_Z4 = A4 * (1 - A4)\n",
    "dA4_nach_Z4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d3cba-896d-4550-8c90-4e732061efd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e00caf-7cd7-4827-a7b1-fae586057aae",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_4}{\\partial w_8} = \\frac{\\partial }{\\partial w_8}(w_7 \\cdot A_1 + w_8 \\cdot A_2 + b_2) = A_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bbac3-4f22-48e2-967f-446b72499e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dZ4_nach_w8 = A2\n",
    "dZ4_nach_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac7bbc-8534-498a-94e1-57f36137c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w8 = dE_total_nach_A4 * dA4_nach_Z4 * dZ4_nach_w8\n",
    "dE_total_nach_w8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6334115-12af-4345-9228-ae484e290667",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_8$ anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006aef5-defd-4f2a-b494-51fc80b2738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w8_neu = w8 - 0.5 * dE_total_nach_w8\n",
    "w8_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ee385-1edf-4646-ac17-61d56cbe01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w8\n",
    "delta_w8 = w8_neu - w8\n",
    "delta_w8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385255ac-5d19-4a01-b1da-3ce5e83b7499",
   "metadata": {},
   "source": [
    "## Versteckte Schicht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918ab42-2b0a-49e2-b30e-f07d326abe61",
   "metadata": {},
   "source": [
    "In der versteckten Schicht ergibt sich eine zusätzliche Abhängigkeit da sowohl $E_1$ als auch $E_2$ von $A_1$ und $A_2$ abhängen, ergibt sich $E_{total}$ zu: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26c918-8b17-44c1-b989-3caffc465479",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Änderung von $E_{total}$ nach $w_1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a14b14-6537-4643-99f9-a965ddf64ac4",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w1_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9974200-8313-403d-8f88-31d7601bfb56",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_1}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d86eac-2e6a-4f57-83dc-b3bcae422926",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_1} = \\frac{\\partial E_1}{\\partial A_1} +  \\frac{\\partial E_2}{\\partial A_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0edbfe-1c9f-47ba-9c6d-e98438b25d5f",
   "metadata": {},
   "source": [
    "weil gilt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35818ae-ca51-489d-ab9f-d193f5de3411",
   "metadata": {},
   "source": [
    "$E_1 = \\frac{1}{2} (\\hat y_1 - A_3 )^2 \\\\ = \\frac{1}{2} (0.01 - A_3 )^2 \\\\ = \\frac{1}{2} (0.01 - sigmoid(Z3) )^2 \\\\ = \\frac{1}{2} (0.01 - sigmoid(w_5 \\cdot A_1 + w_6 \\cdot A_2 + b_2) )^2 \\\\ = E_1 (A_1, A_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3dd50-23a9-43ab-af5f-dd18a87bb7cf",
   "metadata": {},
   "source": [
    "und"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093b39d-4ae0-47ef-8e44-e5dea23a8b71",
   "metadata": {},
   "source": [
    "$E_2 = \\frac{1}{2} (\\hat y_2 - A_4 )^2 \\\\ = \\frac{1}{2} (0.99 - A_4 )^2 \\\\ = \\frac{1}{2} (0.99 - sigmoid(Z4) )^2 \\\\ = \\frac{1}{2} (0.99 - sigmoid(w_7 \\cdot A_1 + w_8 \\cdot A_2 + b_2) )^2 \\\\ = E_2 (A_1, A_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e1d38-2795-4210-83a7-4ceb310a3478",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c21d05-27fc-4a99-8fc6-42128ee27641",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial A_1} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382029aa-eb93-4da5-87ad-8f632e1541c7",
   "metadata": {},
   "source": [
    "Wir haben $\\frac{\\partial E_1}{\\partial Z_3}$ bereits ausgerechnet da gilt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434df462-aa53-4c3d-a7ac-847872672841",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial Z_3} = \\frac{\\partial E_1}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023a2c5-163a-411c-887a-dd182a49610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_nach_Z3 = dE_total_nach_A3 * dA3_nach_Z3\n",
    "dE1_nach_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4a502-fef3-496c-b58c-b70214ca4679",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_1}$ ergibt sich zu:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc94ea7-0335-4775-8df2-f19b49e27596",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_1} = \\frac{\\partial }{\\partial A_1} (w_5 A_1 + w_6 A_2 + b_2) = w_5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d4704-c3f7-4f3a-870d-c280f7f45858",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_nach_A1 = w5\n",
    "dZ3_nach_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94116621-8b72-48fc-b431-41ce52a0c1ec",
   "metadata": {},
   "source": [
    "Insgesamt ergibt sich also für $\\frac{\\partial E_1}{\\partial A_1} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ecd25-3357-4e10-bf4b-c96037786666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_nach_A1 = dE1_nach_Z3 * dZ3_nach_A1\n",
    "dE1_nach_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a0f5b-ca88-495e-80ec-076e193c74c5",
   "metadata": {},
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f68872-fb4b-43bf-a11a-74ae30376020",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_2}{\\partial A_1} = \\frac{\\partial E_2}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial A_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1324a-5b20-4f08-8328-eb493b56566c",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial Z_4} = \\frac{\\partial E_1}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5b39a-ca76-4038-b623-f4c22aaaf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_nach_Z4 = dE_total_nach_A4 * dA4_nach_Z4\n",
    "dE2_nach_Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351e4c4-bff1-4c1d-beb7-740e69bb9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ4_nach_A1 = w7\n",
    "dZ4_nach_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1034c-0f3b-4bfb-b1e5-ea23a56012e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_nach_A1 = dE2_nach_Z4 * dZ4_nach_A1\n",
    "dE2_nach_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66419dd3-3c6c-4cf8-9ff8-6b952200c5f3",
   "metadata": {},
   "source": [
    "Insgesamt ergibt sich für $\\frac{\\partial E_{total}}{\\partial A_1} = \\frac{\\partial E_1}{\\partial A_1} +  \\frac{\\partial E_2}{\\partial A_1} $ also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb00423-c068-418d-ba9b-43e5ecedcd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_A1 = dE1_nach_A1 + dE2_nach_A1\n",
    "dE_total_nach_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cf909-6fd1-499e-a463-7ec3ffc29a8c",
   "metadata": {},
   "source": [
    "Konzentrieren wir uns wieder auf die Ausgangsgleichung: $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_1}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969da03b-a499-4dac-8d29-442b583b81de",
   "metadata": {},
   "source": [
    "Wir benötigen noch $\\frac{\\partial A_1}{\\partial Z_1}$ und $\\frac{\\partial Z_1}{\\partial w_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9e9d3-e0c0-4aff-8982-1a39b77a914a",
   "metadata": {},
   "source": [
    "$\\frac{\\partial A_1}{\\partial Z_1} = A_1 (1 - A_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c97f2c-631a-4ea5-8579-110ae9610933",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA1_nach_Z1 = A1 * (1 - A1)\n",
    "dA1_nach_Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e877c-fa19-493a-9f2f-c1ce616fcc8b",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_1}{\\partial w_1} = \\frac{\\partial Z_1}{\\partial w_1} (w_1 X_1 + w_2 X_2 + b_1) = X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5101ba-3200-47d0-aac3-fa4dab936542",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ1_nach_w1 = X1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e20892-87dd-4fbe-8d00-ee5876246fd6",
   "metadata": {},
   "source": [
    "Insgesamt ergibt sich für $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_1}   $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511dc58-ec35-4bbd-afff-6451a404c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w1 = dE_total_nach_A1 * dA1_nach_Z1 * dZ1_nach_w1\n",
    "dE_total_nach_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315f2d2-d810-4e1e-9ef0-67c5777fe5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_1$ anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e69b5-1094-4b52-bc59-8450928cbf5d",
   "metadata": {},
   "source": [
    "Wir können jetzt das Gewicht $w_1$ mit $w_{1 neu} = w_1 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_1}$ anpassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19faf7c1-edab-4ee3-8459-05f431f41a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_neu = w1 - 0.5 * dE_total_nach_w1\n",
    "w1_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3a472-9d87-418b-8d29-c405a2d13bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w1\n",
    "delta_w1 = w1_neu - w1\n",
    "delta_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027e1b3-a32e-4f1b-8389-e10191992bd4",
   "metadata": {},
   "source": [
    "### Änderung von $E_{total}$ nach $w_2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae305d-0067-4ea2-9504-48412774dbde",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w2_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e906c-2428-4c0a-9729-5a8e62156fd1",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_2}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc438165-fa74-41e3-95a9-0a94e7381664",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_1} = \\frac{\\partial E_1}{\\partial A_1} +  \\frac{\\partial E_2}{\\partial A_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2cd7a-bb57-4d64-a6f2-b0fb1cba87de",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_1}{\\partial w_2} = \\frac{\\partial Z_1}{\\partial w_2} (w_1 X_1 + w_2 X_2 + b_1) = X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c306a7-c55c-4d2f-a2d0-f0b2c0594691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ1_nach_w2 = X2\n",
    "dZ1_nach_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a529b2c-255f-41a8-b88d-d0c973e4285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w2 = dE_total_nach_A1 * dA1_nach_Z1 * dZ1_nach_w2\n",
    "dE_total_nach_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1b425-70d8-498f-b591-0108804cae80",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_2$ anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1406d046-be30-493c-b93c-dfeacd0ae595",
   "metadata": {},
   "source": [
    "Wir können jetzt das Gewicht $w_1$ mit $w_{1 neu} = w_1 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_1}$ anpassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72663f5-3dc1-46c2-a9c5-9ad9cb7c7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_neu = w2 - 0.5 * dE_total_nach_w2\n",
    "w2_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a39ce-725e-4648-bb40-2646bd8eab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w2\n",
    "delta_w2 = w2_neu - w2\n",
    "delta_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2e722-79a3-4665-8b14-b23ecdb5841f",
   "metadata": {},
   "source": [
    "### Änderung von $E_{total}$ nach $w_3$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa1d5e-006a-43c6-a7ff-9e8c045f4bfc",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w3_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324e36d-62c4-427e-b8a6-04e54e44cf7b",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_3}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed096a-b282-45f6-8758-0bcd19ccdd80",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_2} = \\frac{\\partial E_1}{\\partial A_2} +  \\frac{\\partial E_2}{\\partial A_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab6d7f-ecdd-4e79-95c9-f167b47ccf6c",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d794-4537-4411-9bb4-1023424ef619",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial A_2} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14aef5d-b8af-4e24-95c6-5a957f4045bb",
   "metadata": {},
   "source": [
    "Wir haben $\\frac{\\partial E_1}{\\partial Z_3}$ bereits ausgerechnet da gilt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d42ac5-73d4-4ce2-9e6e-7bd3694b8163",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial Z_3} = \\frac{\\partial E_1}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe791f5-d3d6-4721-b835-7f5b90cb21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_nach_Z3 = dE_total_nach_A3 * dA3_nach_Z3\n",
    "dE1_nach_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42c49b-f3f8-4723-868a-b5ffc95eca27",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_2}$ ergibt sich zu:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d98c9-8c6e-4c79-9761-ee387f487be8",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_2} = \\frac{\\partial }{\\partial A_2} (w_5 A_1 + w_6 A_2 + b_2) = w_6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213eaeb3-c8b8-41ea-ab44-760ab335a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_nach_A2 = w6\n",
    "dZ3_nach_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56876f4f-492c-43d2-a918-6d425d01e3a5",
   "metadata": {},
   "source": [
    "Insgesamt ergibt sich also für $\\frac{\\partial E_1}{\\partial A_2} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50628427-7c69-4a2d-9437-9d2f7496671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_nach_A2 = dE1_nach_Z3 * dZ3_nach_A2\n",
    "dE1_nach_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af12c6-636b-4f08-bfbc-e1f4f7c9679f",
   "metadata": {},
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5984bc-90ce-4a1e-85cb-e5660bc602e7",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_2}{\\partial A_2} = \\frac{\\partial E_2}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial A_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156525f3-3bb9-4cea-870d-3d6b4d819ab8",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_2}{\\partial Z_4} = \\frac{\\partial E_2}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c3c70-b444-4602-b691-c2fb04cce825",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_nach_Z4 = dE_total_nach_A4 * dA4_nach_Z4\n",
    "dE2_nach_Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc23141-76c7-44bc-b7a3-3b12b8285f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ4_nach_A2 = w8\n",
    "dZ4_nach_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c08d3a-fae1-403e-89c3-a329b02aa2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_nach_A2 = dE2_nach_Z4 * dZ4_nach_A2\n",
    "dE2_nach_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7cc7f-797d-4a75-ae53-5bb60ad418eb",
   "metadata": {},
   "source": [
    "Insgesamt ergibt sich für $\\frac{\\partial E_{total}}{\\partial A_2} = \\frac{\\partial E_1}{\\partial A_2} +  \\frac{\\partial E_2}{\\partial A_2} $ also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61085476-dcbd-45ae-8dcb-e495ec73520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_A2 = dE1_nach_A2 + dE2_nach_A2\n",
    "dE_total_nach_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305f184-8265-42f4-8009-47de627c0833",
   "metadata": {},
   "source": [
    "Konzentrieren wir uns wieder auf die Ausgangsgleichung: $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_3}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0c585-8ed6-411e-92c5-c91b02842728",
   "metadata": {},
   "source": [
    "Wir benötigen noch $\\frac{\\partial A_2}{\\partial Z_2}$ und $\\frac{\\partial Z_2}{\\partial w_3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e329be8-31fc-44e2-bb15-651679abcc0e",
   "metadata": {},
   "source": [
    "$\\frac{\\partial A_2}{\\partial Z_2} = A_2 (1 - A_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4907b7-b584-424e-8281-288934df3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA2_nach_Z2 = A2 * (1 - A2)\n",
    "dA2_nach_Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d711d-49fd-412f-aa6a-9e6dbf345300",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_2}{\\partial w_3} = \\frac{\\partial }{\\partial w_3} (w_3 X_1 + w_4 X_2 + b_1) = X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9796f4-cb03-47a0-95ec-1535b92ab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ2_nach_w3 = X1\n",
    "dZ2_nach_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1705dc-b9c0-4e1e-b046-44f9fd94eb0b",
   "metadata": {},
   "source": [
    "Insgesamt ergibt sich für $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_3}   $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9b3dd-9a81-4211-81bf-fe584d06b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w3 = dE_total_nach_A2 * dA2_nach_Z2 * dZ2_nach_w3\n",
    "dE_total_nach_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c410ef0-6617-4005-87ae-ee0176c7303b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_3$ anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f3e38-c108-4f6a-8495-00ff3b9216c4",
   "metadata": {},
   "source": [
    "Wir können jetzt das Gewicht $w_3$ mit $w_{3 neu} = w_3 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_3}$ anpassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00257f-ae00-4931-8076-8489c2e83444",
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_neu = w3 - 0.5 * dE_total_nach_w3\n",
    "w3_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec6830-0e4b-4d42-b0a4-caf4b6905c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w3\n",
    "delta_w3 = w3_neu - w3\n",
    "delta_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f099d2d-bf04-4c0d-8189-4acde3de1b59",
   "metadata": {},
   "source": [
    "### Änderung von $E_{total}$ nach $w_4$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae266a3-4f77-47c4-b263-14bcd43ee364",
   "metadata": {},
   "source": [
    "<img src=\"./images/backprop_w4_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3b17b-3f71-46e7-b6dc-8cd470cbc82d",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_4}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71559d-1761-4251-9cd1-1113c5b356d8",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_2} = \\frac{\\partial E_1}{\\partial A_2} +  \\frac{\\partial E_2}{\\partial A_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee9e5f-5fa0-4cb9-99fe-5565bb93d62e",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_2}{\\partial w_4} = \\frac{\\partial Z_2}{\\partial w_4} (w_3 X_1 + w_4 X_2 + b_1) = X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42383a3-fddb-4ef9-a3d0-9c7d17f4c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ2_nach_w4 = X2\n",
    "dZ2_nach_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b22c78-e01d-48c2-a1ea-fe49bb2d4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_nach_w4 = dE_total_nach_A2 * dA2_nach_Z2 * dZ2_nach_w4\n",
    "dE_total_nach_w4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69903aa8-aa5c-477b-ab7c-9c29620d2274",
   "metadata": {},
   "source": [
    "Wir können jetzt das Gewicht $w_4$ mit $w_{4 neu} = w_4 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_4}$ anpassen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2cf22-f55a-4e00-aa6d-6d124b43b4cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gewicht $w_4$ anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfa06c-09e1-4009-8a1b-c6fd2f71150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w4_neu = w4 - 0.5 * dE_total_nach_w4\n",
    "w4_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdcc42-89de-46cb-ba97-99ed907c55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Änderung für w4\n",
    "delta_w4 = w4_neu - w4\n",
    "delta_w4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9e482-220c-4384-8f57-1479678764d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
